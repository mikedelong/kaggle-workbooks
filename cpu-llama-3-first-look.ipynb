{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28d7df9",
   "metadata": {
    "papermill": {
     "duration": 0.003158,
     "end_time": "2024-06-19T17:39:08.251222",
     "exception": false,
     "start_time": "2024-06-19T17:39:08.248064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit where credit is due: this is based on  https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d858492",
   "metadata": {
    "papermill": {
     "duration": 0.004354,
     "end_time": "2024-06-19T17:39:08.258488",
     "exception": false,
     "start_time": "2024-06-19T17:39:08.254134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the model will take a while so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745a64e6",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:39:08.265944Z",
     "iopub.status.busy": "2024-06-19T17:39:08.265554Z",
     "iopub.status.idle": "2024-06-19T17:41:10.219066Z",
     "shell.execute_reply": "2024-06-19T17:41:10.217809Z"
    },
    "papermill": {
     "duration": 121.960266,
     "end_time": "2024-06-19T17:41:10.221790",
     "exception": false,
     "start_time": "2024-06-19T17:39:08.261524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=3663689 sha256=dc5a60c7328a498adc383c2e93116430314d472ac456110b95c6054e57da76fe\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\r\n"
     ]
    }
   ],
   "source": [
    "# clean up after failed efforts if necessary\n",
    "!rm -rf llama.cpp\n",
    "# we need no CUDA for CPU; this flag has changed since the initial release\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=off\" pip install llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4275bbcb",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:41:10.239607Z",
     "iopub.status.busy": "2024-06-19T17:41:10.238494Z",
     "iopub.status.idle": "2024-06-19T17:41:54.628668Z",
     "shell.execute_reply": "2024-06-19T17:41:54.627408Z"
    },
    "papermill": {
     "duration": 44.402167,
     "end_time": "2024-06-19T17:41:54.631573",
     "exception": false,
     "start_time": "2024-06-19T17:41:10.229406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 44.2420 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "from llama_cpp import Llama\n",
    "\n",
    "time_load = arrow.now()\n",
    "llm = Llama(\n",
    "    model_path=\"/kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf\",\n",
    "    n_ctx=4096,      # Max tokens for in + out\n",
    "    n_threads=4,     # CPU cores used\n",
    "    n_gpu_layers=-1,  # Load all layers into VRAM of the GPU\n",
    ")\n",
    "print(\"Model load time: {:5.4f} seconds\".format((arrow.now() - time_load).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebde725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:41:54.669990Z",
     "iopub.status.busy": "2024-06-19T17:41:54.669424Z",
     "iopub.status.idle": "2024-06-19T17:42:06.465822Z",
     "shell.execute_reply": "2024-06-19T17:42:06.464595Z"
    },
    "papermill": {
     "duration": 11.81842,
     "end_time": "2024-06-19T17:42:06.468295",
     "exception": false,
     "start_time": "2024-06-19T17:41:54.649875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4698.01 ms\n",
      "llama_print_timings:      sample time =      44.10 ms /    20 runs   (    2.21 ms per token,   453.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4697.77 ms /    15 tokens (  313.18 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6996.55 ms /    19 runs   (  368.24 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:       total time =   11770.51 ms /    34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the names of three European capital cities?\n",
      "['1. Paris (France), 2. Rome (Italy), 3. Berlin (Germany)']\n",
      "time: 11.7883\n"
     ]
    }
   ],
   "source": [
    "def ask(query: str, max_tokens: int):\n",
    "    time_ask = arrow.now()\n",
    "    output = llm(prompt=\"Q: {} A: \".format(query), max_tokens=max_tokens, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    return [output['choices'][index]['text'] for index in range(len(output['choices']))], (arrow.now() - time_ask).total_seconds()\n",
    "\n",
    "query_cities = 'What are the names of three European capital cities?'\n",
    "result_cities, time_cities = ask(query=query_cities, max_tokens=40)\n",
    "print(query_cities)\n",
    "print(result_cities)\n",
    "print('time: {:5.4f}'.format(time_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776459d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:42:06.507996Z",
     "iopub.status.busy": "2024-06-19T17:42:06.506921Z",
     "iopub.status.idle": "2024-06-19T17:43:27.739014Z",
     "shell.execute_reply": "2024-06-19T17:43:27.737972Z"
    },
    "papermill": {
     "duration": 81.273393,
     "end_time": "2024-06-19T17:43:27.760565",
     "exception": false,
     "start_time": "2024-06-19T17:42:06.487172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4698.01 ms\n",
      "llama_print_timings:      sample time =     480.92 ms /   197 runs   (    2.44 ms per token,   409.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6001.41 ms /    24 tokens (  250.06 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:        eval time =   74400.58 ms /   196 runs   (  379.59 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:       total time =   81217.53 ms /   220 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you tell me about the J. D. Salinger novel The Catcher in the Rye?\n",
      "['1) The Catcher in the Rye is a coming-of-age story written by J.D. Salinger, published in 1951. It follows the journey of Holden Caulfield, a 16-year-old boy who struggles to find his place in society after being expelled from school. 2) The novel explores themes such as alienation, rebellion, and the challenges of adolescence, which are still relevant today. 3) The title \"The Catcher in the Rye\" is often misinterpreted; it refers to Holden\\'s desire to protect children from falling off a cliff, symbolizing his concern for the innocence and vulnerability of young people. 4) The novel has become a classic of American literature, with its protagonist, Holden Caulfield, becoming an iconic figure in Western cultural consciousness. 5) The Catcher in the Rye is known for its simple yet powerful prose, which captures the voice and perspective of a rebellious teenager struggling to find his identity.']\n",
      "time: 81.2258\n"
     ]
    }
   ],
   "source": [
    "query_catcher = 'What can you tell me about the J. D. Salinger novel The Catcher in the Rye?'\n",
    "result_catcher, time_catcher = ask(query=query_catcher, max_tokens=1000)\n",
    "print(query_catcher)\n",
    "print(result_catcher)\n",
    "print('time: {:5.4f}'.format(time_catcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fc1eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:43:27.800385Z",
     "iopub.status.busy": "2024-06-19T17:43:27.799947Z",
     "iopub.status.idle": "2024-06-19T17:43:38.350779Z",
     "shell.execute_reply": "2024-06-19T17:43:38.349484Z"
    },
    "papermill": {
     "duration": 10.575573,
     "end_time": "2024-06-19T17:43:38.354881",
     "exception": false,
     "start_time": "2024-06-19T17:43:27.779308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4698.01 ms\n",
      "llama_print_timings:      sample time =      38.07 ms /    17 runs   (    2.24 ms per token,   446.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4703.26 ms /    15 tokens (  313.55 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:        eval time =    5772.11 ms /    16 runs   (  360.76 ms per token,     2.77 tokens per second)\n",
      "llama_print_timings:       total time =   10537.00 ms /    31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 hours and 30 minutes by car, or take the train. (more...)']\n"
     ]
    }
   ],
   "source": [
    "r, t = ask(query='How do I drive from Washington DC to New York City?', max_tokens=3000)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba598d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:43:38.396127Z",
     "iopub.status.busy": "2024-06-19T17:43:38.395253Z",
     "iopub.status.idle": "2024-06-19T17:44:01.887513Z",
     "shell.execute_reply": "2024-06-19T17:44:01.886358Z"
    },
    "papermill": {
     "duration": 23.515981,
     "end_time": "2024-06-19T17:44:01.890306",
     "exception": false,
     "start_time": "2024-06-19T17:43:38.374325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4698.01 ms\n",
      "llama_print_timings:      sample time =     121.37 ms /    54 runs   (    2.25 ms per token,   444.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3800.92 ms /    15 tokens (  253.39 ms per token,     3.95 tokens per second)\n",
      "llama_print_timings:        eval time =   19481.20 ms /    53 runs   (  367.57 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:       total time =   23478.05 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' While the idea of building an edible bicycle might be intriguing, it is important to note that bicycles are typically made of materials such as metal, plastic, and other synthetic substances. These materials are not edible and cannot be used to create a functional bicycle that can be consumed.']\n"
     ]
    }
   ],
   "source": [
    "r0, t0 = ask(query='What would I need to do to build an edible bicycle?', max_tokens=3000)\n",
    "print(r0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4880117,
     "sourceId": 8230136,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 298.687059,
   "end_time": "2024-06-19T17:44:02.933513",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-19T17:39:04.246454",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
