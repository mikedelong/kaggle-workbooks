{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd043d4",
   "metadata": {
    "papermill": {
     "duration": 0.004738,
     "end_time": "2024-06-18T18:47:48.551755",
     "exception": false,
     "start_time": "2024-06-18T18:47:48.547017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit where credit is due: this is based on  https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fb1c7",
   "metadata": {
    "papermill": {
     "duration": 0.004705,
     "end_time": "2024-06-18T18:47:48.561159",
     "exception": false,
     "start_time": "2024-06-18T18:47:48.556454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the model will take a while so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aba788c",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:48.568525Z",
     "iopub.status.busy": "2024-06-18T18:47:48.568111Z",
     "iopub.status.idle": "2024-06-18T18:49:51.817653Z",
     "shell.execute_reply": "2024-06-18T18:49:51.816230Z"
    },
    "papermill": {
     "duration": 123.256423,
     "end_time": "2024-06-18T18:49:51.820458",
     "exception": false,
     "start_time": "2024-06-18T18:47:48.564035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=3658465 sha256=501a80a6c19ac98914810414df87060c335f6593c8dd3b649273a7f056cb4a58\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\r\n"
     ]
    }
   ],
   "source": [
    "# clean up after failed efforts if necessary\n",
    "!rm -rf llama.cpp\n",
    "# we need no CUDA for CPU; this flag has changed since the initial release\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=off\" pip install llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098aceb9",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:49:51.836805Z",
     "iopub.status.busy": "2024-06-18T18:49:51.836411Z",
     "iopub.status.idle": "2024-06-18T18:50:57.891028Z",
     "shell.execute_reply": "2024-06-18T18:50:57.889606Z"
    },
    "papermill": {
     "duration": 66.066041,
     "end_time": "2024-06-18T18:50:57.893669",
     "exception": false,
     "start_time": "2024-06-18T18:49:51.827628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 65.9038 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "from llama_cpp import Llama\n",
    "\n",
    "time_load = arrow.now()\n",
    "# llm = Llama(model_path=\"/kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf\")\n",
    "llm = Llama(\n",
    "    model_path=\"/kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf\",\n",
    "    n_ctx=4096,      # Max tokens for in + out\n",
    "    n_threads=4,     # CPU cores used\n",
    "    n_gpu_layers=-1,  # Load all layers into VRAM of the GPU\n",
    ")\n",
    "print(\"Model load time: {:5.4f} seconds\".format((arrow.now() - time_load).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a64a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:50:57.933121Z",
     "iopub.status.busy": "2024-06-18T18:50:57.932553Z",
     "iopub.status.idle": "2024-06-18T18:51:09.931347Z",
     "shell.execute_reply": "2024-06-18T18:51:09.929332Z"
    },
    "papermill": {
     "duration": 12.021513,
     "end_time": "2024-06-18T18:51:09.933740",
     "exception": false,
     "start_time": "2024-06-18T18:50:57.912227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4843.59 ms\n",
      "llama_print_timings:      sample time =      51.08 ms /    22 runs   (    2.32 ms per token,   430.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4843.47 ms /    16 tokens (  302.72 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:        eval time =    7049.44 ms /    21 runs   (  335.69 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:       total time =   11978.64 ms /    37 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the names of three European capital cities?\n",
      "['1) Paris, France; 2) London, United Kingdom; and 3) Rome, Italy.']\n",
      "time: 11.9889\n"
     ]
    }
   ],
   "source": [
    "def ask(query: str, max_tokens: int):\n",
    "    time_ask = arrow.now()\n",
    "    output = llm(\"Q: {} A: \".format(query), max_tokens=max_tokens, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    return [output['choices'][index]['text'] for index in range(len(output['choices']))], (arrow.now() - time_ask).total_seconds()\n",
    "\n",
    "query_cities = 'What are the names of three European capital cities?'\n",
    "result_cities, time_cities = ask(query=query_cities, max_tokens=40)\n",
    "print(query_cities)\n",
    "print(result_cities)\n",
    "print('time: {:5.4f}'.format(time_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855dfe74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:51:09.975543Z",
     "iopub.status.busy": "2024-06-18T18:51:09.974760Z",
     "iopub.status.idle": "2024-06-18T18:52:39.568224Z",
     "shell.execute_reply": "2024-06-18T18:52:39.567084Z"
    },
    "papermill": {
     "duration": 89.637236,
     "end_time": "2024-06-18T18:52:39.590921",
     "exception": false,
     "start_time": "2024-06-18T18:51:09.953685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4843.59 ms\n",
      "llama_print_timings:      sample time =     576.18 ms /   234 runs   (    2.46 ms per token,   406.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6029.25 ms /    24 tokens (  251.22 ms per token,     3.98 tokens per second)\n",
      "llama_print_timings:        eval time =   82581.36 ms /   233 runs   (  354.43 ms per token,     2.82 tokens per second)\n",
      "llama_print_timings:       total time =   89579.45 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you tell me about the J. D. Salinger novel The Catcher in the Rye?\n",
      "['1) Published in 1951, this coming-of-age story follows the protagonist, Holden Caulfield, as he navigates his way through a challenging day in New York City after being expelled from a prestigious boarding school. Throughout the novel, Holden grapples with feelings of alienation, rebellion, and disillusionment, leading him to question the phoniness of the adult world and the superficiality of those around him. 2) The title \"The Catcher in the Rye\" is derived from a line in Robert Burns\\' poem \"Comin\\' Through the Rye,\" which Holden misquotes as \"If a body catch a body coming through the rye.\" This phrase symbolizes his desire to protect children from falling off a cliff, representing his fear of losing innocence and becoming corrupt like the adults he despises. 3) The novel has been widely interpreted as an exploration of adolescent angst, exploring themes such as identity formation, rebellion against societal expectations, and the struggle for autonomy. Despite its initial controversy due to perceived obscenity, \"The Catcher in the Rye\" has become a classic of American literature and continues to resonate with readers worldwide.']\n",
      "time: 89.5869\n"
     ]
    }
   ],
   "source": [
    "query_catcher = 'What can you tell me about the J. D. Salinger novel The Catcher in the Rye?'\n",
    "result_catcher, time_catcher = ask(query=query_catcher, max_tokens=1000)\n",
    "print(query_catcher)\n",
    "print(result_catcher)\n",
    "print('time: {:5.4f}'.format(time_catcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01103711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:52:39.635052Z",
     "iopub.status.busy": "2024-06-18T18:52:39.634633Z",
     "iopub.status.idle": "2024-06-18T18:52:59.495021Z",
     "shell.execute_reply": "2024-06-18T18:52:59.493803Z"
    },
    "papermill": {
     "duration": 19.884225,
     "end_time": "2024-06-18T18:52:59.497392",
     "exception": false,
     "start_time": "2024-06-18T18:52:39.613167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4843.59 ms\n",
      "llama_print_timings:      sample time =     106.38 ms /    48 runs   (    2.22 ms per token,   451.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3792.90 ms /    15 tokens (  252.86 ms per token,     3.95 tokens per second)\n",
      "llama_print_timings:        eval time =   15883.38 ms /    47 runs   (  337.94 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =   19846.21 ms /    62 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\xa0Driving from Washington, D.C. to New York City can be a bit of a challenge due to traffic and construction, but it's definitely doable. Here are the steps you can follow to make your journey as smooth as possible:\"]\n"
     ]
    }
   ],
   "source": [
    "r, t = ask(query='How do I drive from Washington DC to New York City?', max_tokens=3000)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4eb509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:52:59.539586Z",
     "iopub.status.busy": "2024-06-18T18:52:59.538758Z",
     "iopub.status.idle": "2024-06-18T18:53:13.093656Z",
     "shell.execute_reply": "2024-06-18T18:53:13.092258Z"
    },
    "papermill": {
     "duration": 13.578609,
     "end_time": "2024-06-18T18:53:13.096107",
     "exception": false,
     "start_time": "2024-06-18T18:52:59.517498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4843.59 ms\n",
      "llama_print_timings:      sample time =      57.51 ms /    26 runs   (    2.21 ms per token,   452.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3792.21 ms /    15 tokens (  252.81 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9657.58 ms /    25 runs   (  386.30 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =   13539.82 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. Choose the right materials: You'll want a durable and food-safe material for your bicycle, like stainless steel or bamboo.\"]\n"
     ]
    }
   ],
   "source": [
    "r0, t0 = ask(query='What would I need to do to build an edible bicycle?', max_tokens=3000)\n",
    "print(r0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4880117,
     "sourceId": 8230136,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 330.088184,
   "end_time": "2024-06-18T18:53:14.141768",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-18T18:47:44.053584",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
