{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9bce62",
   "metadata": {
    "papermill": {
     "duration": 0.003162,
     "end_time": "2024-06-19T17:43:24.283220",
     "exception": false,
     "start_time": "2024-06-19T17:43:24.280058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit where credit is due: this is based on  https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f118440",
   "metadata": {
    "papermill": {
     "duration": 0.004057,
     "end_time": "2024-06-19T17:43:24.290053",
     "exception": false,
     "start_time": "2024-06-19T17:43:24.285996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the model will take a while so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadbf893",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:43:24.296921Z",
     "iopub.status.busy": "2024-06-19T17:43:24.296501Z",
     "iopub.status.idle": "2024-06-19T17:45:22.496525Z",
     "shell.execute_reply": "2024-06-19T17:45:22.495182Z"
    },
    "papermill": {
     "duration": 118.206741,
     "end_time": "2024-06-19T17:45:22.499364",
     "exception": false,
     "start_time": "2024-06-19T17:43:24.292623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=3663687 sha256=5587693214008f89cddde90677b7140e78b51bf169ab26b763700140f97e39f6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\r\n"
     ]
    }
   ],
   "source": [
    "# clean up after failed efforts if necessary\n",
    "!rm -rf llama.cpp\n",
    "# we need no CUDA for CPU; this flag has changed since the initial release\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=off\" pip install llama-cpp-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928a4104",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:45:22.516600Z",
     "iopub.status.busy": "2024-06-19T17:45:22.516196Z",
     "iopub.status.idle": "2024-06-19T17:46:19.270197Z",
     "shell.execute_reply": "2024-06-19T17:46:19.269023Z"
    },
    "papermill": {
     "duration": 56.767323,
     "end_time": "2024-06-19T17:46:19.274079",
     "exception": false,
     "start_time": "2024-06-19T17:45:22.506756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 56.6172 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "from llama_cpp import Llama\n",
    "\n",
    "time_load = arrow.now()\n",
    "llm = Llama(\n",
    "    model_path=\"/kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf\",\n",
    "    n_ctx=4096,      # Max tokens for in + out\n",
    "    n_threads=4,     # CPU cores used\n",
    "    n_gpu_layers=-1,  # Load all layers into VRAM of the GPU\n",
    ")\n",
    "print(\"Model load time: {:5.4f} seconds\".format((arrow.now() - time_load).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae572428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:46:19.317030Z",
     "iopub.status.busy": "2024-06-19T17:46:19.316416Z",
     "iopub.status.idle": "2024-06-19T17:46:30.512203Z",
     "shell.execute_reply": "2024-06-19T17:46:30.511164Z"
    },
    "papermill": {
     "duration": 11.218688,
     "end_time": "2024-06-19T17:46:30.515366",
     "exception": false,
     "start_time": "2024-06-19T17:46:19.296678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4075.66 ms\n",
      "llama_print_timings:      sample time =      44.06 ms /    20 runs   (    2.20 ms per token,   453.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4075.49 ms /    15 tokens (  271.70 ms per token,     3.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7026.59 ms /    19 runs   (  369.82 ms per token,     2.70 tokens per second)\n",
      "llama_print_timings:       total time =   11177.36 ms /    34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the names of three European capital cities?\n",
      "['1. Paris, France; 2. Rome, Italy; 3. London, UK.']\n",
      "time: 11.1850\n"
     ]
    }
   ],
   "source": [
    "def ask(query: str, max_tokens: int):\n",
    "    time_ask = arrow.now()\n",
    "    output = llm(prompt=\"Q: {} A: \".format(query), max_tokens=max_tokens, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    return [output['choices'][index]['text'] for index in range(len(output['choices']))], (arrow.now() - time_ask).total_seconds()\n",
    "\n",
    "query_cities = 'What are the names of three European capital cities?'\n",
    "result_cities, time_cities = ask(query=query_cities, max_tokens=40)\n",
    "print(query_cities)\n",
    "print(result_cities)\n",
    "print('time: {:5.4f}'.format(time_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08315c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:46:30.554481Z",
     "iopub.status.busy": "2024-06-19T17:46:30.553575Z",
     "iopub.status.idle": "2024-06-19T17:46:36.887251Z",
     "shell.execute_reply": "2024-06-19T17:46:36.886435Z"
    },
    "papermill": {
     "duration": 6.356247,
     "end_time": "2024-06-19T17:46:36.890161",
     "exception": false,
     "start_time": "2024-06-19T17:46:30.533914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4075.66 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /     2 runs   (    2.36 ms per token,   424.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5947.90 ms /    24 tokens (  247.83 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:        eval time =     363.51 ms /     1 runs   (  363.51 ms per token,     2.75 tokens per second)\n",
      "llama_print_timings:       total time =    6320.05 ms /    25 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you tell me about the J. D. Salinger novel The Catcher in the Rye?\n",
      "['1.']\n",
      "time: 6.3283\n"
     ]
    }
   ],
   "source": [
    "query_catcher = 'What can you tell me about the J. D. Salinger novel The Catcher in the Rye?'\n",
    "result_catcher, time_catcher = ask(query=query_catcher, max_tokens=1000)\n",
    "print(query_catcher)\n",
    "print(result_catcher)\n",
    "print('time: {:5.4f}'.format(time_catcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fca15b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:46:36.929813Z",
     "iopub.status.busy": "2024-06-19T17:46:36.929430Z",
     "iopub.status.idle": "2024-06-19T17:47:38.056650Z",
     "shell.execute_reply": "2024-06-19T17:47:38.055593Z"
    },
    "papermill": {
     "duration": 61.173504,
     "end_time": "2024-06-19T17:47:38.082731",
     "exception": false,
     "start_time": "2024-06-19T17:46:36.909227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4075.66 ms\n",
      "llama_print_timings:      sample time =     327.48 ms /   149 runs   (    2.20 ms per token,   454.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3787.99 ms /    15 tokens (  252.53 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:        eval time =   56773.63 ms /   148 runs   (  383.61 ms per token,     2.61 tokens per second)\n",
      "llama_print_timings:       total time =   61114.91 ms /   163 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. From Washington DC, take I-495 N towards Baltimore. 2. Continue on I-495 N for approximately 30 miles until you reach the Capital Beltway. 3. Take the exit for I-95 N/US-1 N towards Baltimore/New York City. 4. Stay on I-95 N for about 140 miles until you reach the George Washington Bridge in New York City. 5. Cross the bridge and take the exit for I-78 W/US-1 S towards Newark. 6. Continue on I-78 W/US-1 S for approximately 10 miles until you reach downtown Newark. From there, you can access various parts of the city using local roads and highways.']\n"
     ]
    }
   ],
   "source": [
    "r0, t0 = ask(query='How do I drive from Washington DC to New York City?', max_tokens=3000)\n",
    "print(r0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fd26b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:47:38.124165Z",
     "iopub.status.busy": "2024-06-19T17:47:38.123724Z",
     "iopub.status.idle": "2024-06-19T17:48:11.204407Z",
     "shell.execute_reply": "2024-06-19T17:48:11.203392Z"
    },
    "papermill": {
     "duration": 33.123286,
     "end_time": "2024-06-19T17:48:11.225917",
     "exception": false,
     "start_time": "2024-06-19T17:47:38.102631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4075.66 ms\n",
      "llama_print_timings:      sample time =     180.66 ms /    77 runs   (    2.35 ms per token,   426.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3734.29 ms /    15 tokens (  248.95 ms per token,     4.02 tokens per second)\n",
      "llama_print_timings:        eval time =   29050.43 ms /    76 runs   (  382.24 ms per token,     2.62 tokens per second)\n",
      "llama_print_timings:       total time =   33067.44 ms /    91 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" That's a unique question! While it might be possible to create a partially edible bicycle using ingredients like sugar, honey, or even chocolate (with some creative molding), building a fully functional and safe edible bicycle would likely require significant advances in food engineering and manufacturing. For now, let's just stick to enjoying our favorite treats as snacks rather than trying to build entire vehicles out of them!\"]\n"
     ]
    }
   ],
   "source": [
    "r1, t1 = ask(query='What would I need to do to build an edible bicycle?', max_tokens=3000)\n",
    "print(r1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4880117,
     "sourceId": 8230136,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 291.784559,
   "end_time": "2024-06-19T17:48:12.169332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-19T17:43:20.384773",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
