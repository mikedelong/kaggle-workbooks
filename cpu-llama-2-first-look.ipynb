{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66cafd6",
   "metadata": {
    "papermill": {
     "duration": 0.004282,
     "end_time": "2024-06-19T17:50:56.590521",
     "exception": false,
     "start_time": "2024-06-19T17:50:56.586239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit where credit is due: this is based on  https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a47c4",
   "metadata": {
    "papermill": {
     "duration": 0.003373,
     "end_time": "2024-06-19T17:50:56.599800",
     "exception": false,
     "start_time": "2024-06-19T17:50:56.596427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the model data takes a while so please be patient waiting for the notebook to start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d78876",
   "metadata": {
    "papermill": {
     "duration": 0.003624,
     "end_time": "2024-06-19T17:50:56.606993",
     "exception": false,
     "start_time": "2024-06-19T17:50:56.603369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First we need to install the llama cpp package; we need the no-CUDA version. If the install fails it will leave stuff in the filespace we need to clean up before we can try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0523b26",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:50:56.616564Z",
     "iopub.status.busy": "2024-06-19T17:50:56.616159Z",
     "iopub.status.idle": "2024-06-19T17:53:10.627274Z",
     "shell.execute_reply": "2024-06-19T17:53:10.625799Z"
    },
    "papermill": {
     "duration": 134.019488,
     "end_time": "2024-06-19T17:53:10.630182",
     "exception": false,
     "start_time": "2024-06-19T17:50:56.610694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=3663678 sha256=23d757cddf8386ba4afd34233e40e66c2a0b846473c521ee94f327f1589be8d5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\r\n",
      "Cloning into 'llama.cpp'...\r\n",
      "remote: Enumerating objects: 27509, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (9122/9122), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (621/621), done.\u001b[K\r\n",
      "remote: Total 27509 (delta 8800), reused 8584 (delta 8496), pack-reused 18387\u001b[K\r\n",
      "Receiving objects: 100% (27509/27509), 48.44 MiB | 23.12 MiB/s, done.\r\n",
      "Resolving deltas: 100% (19694/19694), done.\r\n"
     ]
    }
   ],
   "source": [
    "# clean up after failed efforts if necessary\n",
    "!rm -rf llama.cpp\n",
    "# we need no CUDA for CPU; this flag has changed since the initial release\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=off\" pip install llama-cpp-python \n",
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58a50a",
   "metadata": {
    "papermill": {
     "duration": 0.012853,
     "end_time": "2024-06-19T17:53:10.656018",
     "exception": false,
     "start_time": "2024-06-19T17:53:10.643165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also need to load the quantized version of the 7B model from our local resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d2847d",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T17:53:10.684548Z",
     "iopub.status.busy": "2024-06-19T17:53:10.684111Z",
     "iopub.status.idle": "2024-06-19T17:53:58.588131Z",
     "shell.execute_reply": "2024-06-19T17:53:58.585474Z"
    },
    "papermill": {
     "duration": 47.922388,
     "end_time": "2024-06-19T17:53:58.592016",
     "exception": false,
     "start_time": "2024-06-19T17:53:10.669628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from /kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = 7b-chat-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = 7b-chat-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 47.7547 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '2048', 'general.name': '7b-chat-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "from llama_cpp import Llama\n",
    "\n",
    "time_load = arrow.now()\n",
    "llm = Llama(model_path=\"/kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf\")\n",
    "print(\"Model load time: {:5.4f} seconds\".format((arrow.now() - time_load).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8184a",
   "metadata": {
    "papermill": {
     "duration": 0.026237,
     "end_time": "2024-06-19T17:53:58.644318",
     "exception": false,
     "start_time": "2024-06-19T17:53:58.618081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's wrap up the code to do the query and parse the result in a function. Sometimes the results look weird when parsed, so for the moment we return the underlying object for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ca7658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:53:58.698811Z",
     "iopub.status.busy": "2024-06-19T17:53:58.697976Z",
     "iopub.status.idle": "2024-06-19T17:54:14.285658Z",
     "shell.execute_reply": "2024-06-19T17:54:14.283526Z"
    },
    "papermill": {
     "duration": 15.61953,
     "end_time": "2024-06-19T17:54:14.289848",
     "exception": false,
     "start_time": "2024-06-19T17:53:58.670318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      17.22 ms /    21 runs   (    0.82 ms per token,  1219.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2805.02 ms /    16 tokens (  175.31 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:        eval time =   12706.37 ms /    20 runs   (  635.32 ms per token,     1.57 tokens per second)\n",
      "llama_print_timings:       total time =   15563.92 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the names of three European capital cities?\n",
      "['1. - Paris, France 2. - London, England 3. - Rome, Italy']\n",
      "time: 15.5759\n"
     ]
    }
   ],
   "source": [
    "def ask(query: str, max_tokens: int):\n",
    "    time_ask = arrow.now()\n",
    "    output = llm(prompt=\"Q: {} A: \".format(query), max_tokens=max_tokens, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    return [output['choices'][index]['text'] for index in range(len(output['choices']))], (arrow.now() - time_ask).total_seconds(), output\n",
    "\n",
    "query_cities = 'What are the names of three European capital cities?'\n",
    "result_cities, time_cities, debug_cities = ask(query=query_cities, max_tokens=40)\n",
    "print(query_cities)\n",
    "print(result_cities)\n",
    "print('time: {:5.4f}'.format(time_cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6980c5c",
   "metadata": {
    "papermill": {
     "duration": 0.024276,
     "end_time": "2024-06-19T17:54:14.342674",
     "exception": false,
     "start_time": "2024-06-19T17:54:14.318398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What's interesting is that if we ask this question ten times we get a range of answers, most of the time correct and not weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77393138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:54:14.394444Z",
     "iopub.status.busy": "2024-06-19T17:54:14.393953Z",
     "iopub.status.idle": "2024-06-19T17:55:24.256814Z",
     "shell.execute_reply": "2024-06-19T17:55:24.255717Z"
    },
    "papermill": {
     "duration": 69.922126,
     "end_time": "2024-06-19T17:55:24.289165",
     "exception": false,
     "start_time": "2024-06-19T17:54:14.367039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      11.12 ms /    19 runs   (    0.59 ms per token,  1708.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7156.00 ms /    19 runs   (  376.63 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    7182.01 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Paris, France 2. London, United Kingdom 3. Rome, Italy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      13.52 ms /    22 runs   (    0.61 ms per token,  1627.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    8609.69 ms /    22 runs   (  391.35 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:       total time =    8637.98 ms /    22 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Three European capital cities are London (United Kingdom), Paris (France), and Rome (Italy).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.53 ms /    18 runs   (    0.58 ms per token,  1709.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6707.63 ms /    18 runs   (  372.65 ms per token,     2.68 tokens per second)\n",
      "llama_print_timings:       total time =    6729.01 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Berlin, Germany 2. Paris, France 3. London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      11.21 ms /    19 runs   (    0.59 ms per token,  1694.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7145.85 ms /    19 runs   (  376.10 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    7173.83 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Berlin, Germany 2. Paris, France 3. London, United Kingdom']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.44 ms /    18 runs   (    0.58 ms per token,  1724.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6770.74 ms /    18 runs   (  376.15 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    6793.16 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Paris, France 2. Rome, Italy 3. Madrid, Spain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.62 ms /    18 runs   (    0.59 ms per token,  1694.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7108.81 ms /    18 runs   (  394.93 ms per token,     2.53 tokens per second)\n",
      "llama_print_timings:       total time =    7130.95 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Vienna, Austria 2. Paris, France 3. Rome, Italy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.58 ms /    18 runs   (    0.59 ms per token,  1702.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6772.55 ms /    18 runs   (  376.25 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    6793.87 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Berlin, Germany 2. Paris, France 3. London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.37 ms /    18 runs   (    0.58 ms per token,  1735.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6777.60 ms /    18 runs   (  376.53 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    6798.91 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Paris, France 2. London, England 3. Rome, Italy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.61 ms /    18 runs   (    0.59 ms per token,  1696.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6841.05 ms /    18 runs   (  380.06 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:       total time =    6862.46 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Paris, France 2. London, UK 3. Rome, Italy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =       8.63 ms /    15 runs   (    0.58 ms per token,  1738.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    5655.11 ms /    15 runs   (  377.01 ms per token,     2.65 tokens per second)\n",
      "llama_print_timings:       total time =    5672.50 ms /    15 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    }
   ],
   "source": [
    "for index in range(10):\n",
    "    result_cities, time_cities, _ = ask(query=query_cities, max_tokens=40)\n",
    "    print(result_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6367fea",
   "metadata": {
    "papermill": {
     "duration": 0.128515,
     "end_time": "2024-06-19T17:55:24.447089",
     "exception": false,
     "start_time": "2024-06-19T17:55:24.318574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "But if we phrase the question slightly differently we get the same answer every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745f0aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:55:24.508536Z",
     "iopub.status.busy": "2024-06-19T17:55:24.508109Z",
     "iopub.status.idle": "2024-06-19T17:56:41.408439Z",
     "shell.execute_reply": "2024-06-19T17:56:41.405653Z"
    },
    "papermill": {
     "duration": 76.934087,
     "end_time": "2024-06-19T17:56:41.410983",
     "exception": false,
     "start_time": "2024-06-19T17:55:24.476896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.96 ms /    19 runs   (    0.58 ms per token,  1733.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2141.36 ms /     9 tokens (  237.93 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:        eval time =    6868.00 ms /    18 runs   (  381.56 ms per token,     2.62 tokens per second)\n",
      "llama_print_timings:       total time =    9031.87 ms /    27 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      12.12 ms /    21 runs   (    0.58 ms per token,  1732.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7865.64 ms /    21 runs   (  374.55 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time =    7890.56 ms /    21 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.76 ms /    19 runs   (    0.57 ms per token,  1765.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7193.72 ms /    19 runs   (  378.62 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =    7216.68 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      13.84 ms /    24 runs   (    0.58 ms per token,  1733.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    9065.70 ms /    24 runs   (  377.74 ms per token,     2.65 tokens per second)\n",
      "llama_print_timings:       total time =    9095.38 ms /    24 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      11.08 ms /    19 runs   (    0.58 ms per token,  1715.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7427.57 ms /    19 runs   (  390.92 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:       total time =    7451.49 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.17 ms /    18 runs   (    0.56 ms per token,  1770.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6810.76 ms /    18 runs   (  378.38 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =    6831.81 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      10.54 ms /    18 runs   (    0.59 ms per token,  1706.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    6815.90 ms /    18 runs   (  378.66 ms per token,     2.64 tokens per second)\n",
      "llama_print_timings:       total time =    6837.58 ms /    18 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =       7.59 ms /    13 runs   (    0.58 ms per token,  1712.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    4818.99 ms /    13 runs   (  370.69 ms per token,     2.70 tokens per second)\n",
      "llama_print_timings:       total time =    4834.83 ms /    13 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      11.39 ms /    19 runs   (    0.60 ms per token,  1668.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7454.46 ms /    19 runs   (  392.34 ms per token,     2.55 tokens per second)\n",
      "llama_print_timings:       total time =    7478.36 ms /    19 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      15.67 ms /    27 runs   (    0.58 ms per token,  1723.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   10102.83 ms /    27 runs   (  374.18 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time =   10136.39 ms /    27 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 European Capital Cities Berlin, Germany Paris, France London, UK']\n"
     ]
    }
   ],
   "source": [
    "query_capitals = 'What are three capitals of Europe?'\n",
    "for index in range(10):\n",
    "    result_capitals, time_capitals, debug_capitals = ask(query=query_capitals, max_tokens=40)\n",
    "    print(result_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd25be",
   "metadata": {
    "papermill": {
     "duration": 0.034188,
     "end_time": "2024-06-19T17:56:41.479329",
     "exception": false,
     "start_time": "2024-06-19T17:56:41.445141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For a more open-ended question we may need several tries to get a useful answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0461939f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T17:56:41.549811Z",
     "iopub.status.busy": "2024-06-19T17:56:41.549357Z",
     "iopub.status.idle": "2024-06-19T18:03:32.545475Z",
     "shell.execute_reply": "2024-06-19T18:03:32.544284Z"
    },
    "papermill": {
     "duration": 411.034741,
     "end_time": "2024-06-19T18:03:32.548782",
     "exception": false,
     "start_time": "2024-06-19T17:56:41.514041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =       0.59 ms /     1 runs   (    0.59 ms per token,  1703.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3267.26 ms /    22 tokens (  148.51 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    3269.36 ms /    23 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      94.66 ms /   145 runs   (    0.65 ms per token,  1531.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   55857.20 ms /   145 runs   (  385.22 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:       total time =   56078.11 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The protagonist, Holden Caulfield, runs away from his boarding school and spends three days wandering around New York City, grappling with his disillusionment with society and his sense of alienation from those around him. Along the way, he encounters a variety of characters who challenge or reinforce his worldview, including his former roommate Stradlater, his sister Phoebe, and several women he meets in bars and clubs. Throughout the novel, Holden struggles with feelings of loneliness, anger, and disillusionment, and his experiences raise questions about the nature of identity, morality, and the human condition.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      80.22 ms /   123 runs   (    0.65 ms per token,  1533.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   47048.45 ms /   123 runs   (  382.51 ms per token,     2.61 tokens per second)\n",
      "llama_print_timings:       total time =   47226.48 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' In J.D. Salinger\\'s novel, \"The Catcher in the Rye,\" the protagonist, Holden Caulfield, struggles to navigate the challenges of adolescence and adulthood. He is a disillusioned teenager who has just been expelled from a prestigious boarding school and is struggling to find his place in the world. The novel follows Holden\\'s experiences over the course of three days, during which he grapples with feelings of alienation, loneliness, and disillusionment.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =     125.33 ms /   191 runs   (    0.66 ms per token,  1523.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   73861.36 ms /   191 runs   (  386.71 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =   74157.46 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Holden Caulfield, a disillusioned teenager, runs away from his boarding school and spends three days wandering around New York City, alienating himself from those he meets and reflecting on his feelings of alienation and disgust towards phoniness and superficiality in society. He eventually ends up at a nightclub where he confronts his former roommate and rival, Stradlater, who is on a date with a woman named Judith. Holden becomes drunk and depressed, and eventually returns to his school for Christmas vacation. Along the way, Holden narrates his experiences in a sarcastic and cynical tone, frequently breaking the fourth wall and addressing the reader directly. The novel is known for its themes of teenage angst, rebellion, and the struggle to find one's place in society.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      76.79 ms /   120 runs   (    0.64 ms per token,  1562.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   46357.14 ms /   120 runs   (  386.31 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:       total time =   46528.74 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The Catcher in the Rye is a novel by J.D. Salinger that follows the story of Holden Caulfield, a disillusioned teenager who runs away from his boarding school and spends three days exploring New York City. During this time, he grapples with feelings of alienation and disconnection from society, and struggles to find his place in the world. The novel is known for its candid portrayal of adolescence and its themes of identity, belonging, and the challenges of growing up.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      71.72 ms /   110 runs   (    0.65 ms per token,  1533.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   42383.60 ms /   110 runs   (  385.31 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:       total time =   42541.40 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Holden Caulfield, a disillusioned teenager, runs away from his boarding school and spends three days in New York City, where he grapples with feelings of alienation and disconnection from society. Along the way, he encounters various characters who challenge his perceptions of the world around him. At the end of the novel, Holden is hospitalized after attempting to jump off a bridge, leaving readers to wonder about his fate and whether or not he will ever find happiness.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      70.35 ms /   110 runs   (    0.64 ms per token,  1563.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   42871.90 ms /   110 runs   (  389.74 ms per token,     2.57 tokens per second)\n",
      "llama_print_timings:       total time =   43027.77 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" The Catcher in the Rye is a classic coming-of-age novel by J.D. Salinger, first published in 1951. The story follows the protagonist, Holden Caulfield, a disillusioned teenager who has been expelled from several prestigious boarding schools. The novel explores themes of alienation, rebellion, and the struggle to find one's identity in a phony adult world. Here is a brief summary of the plot:\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      57.99 ms /    90 runs   (    0.64 ms per token,  1551.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   34546.23 ms /    90 runs   (  383.85 ms per token,     2.61 tokens per second)\n",
      "llama_print_timings:       total time =   34671.96 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Holden Caulfield, a disillusioned teenager, narrates his experiences after being expelled from boarding school. He spends three days in New York City, wandering the streets and encountering various characters, including his former friends and a potential love interest. Along the way, he grapples with feelings of alienation and disillusionment, questioning the values and conformity of society.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      62.67 ms /    96 runs   (    0.65 ms per token,  1531.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   36971.51 ms /    96 runs   (  385.12 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:       total time =   37107.95 ms /    96 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Holden Caulfield, a disillusioned teenager, runs away from his boarding school and spends three days wandering around New York City, alienating himself from those he encounters due to his cynical views on society. He eventually ends up in a diner where he has a conversation with a former teacher that reveals the deep-seated feelings of loneliness and disconnection he feels from the world around him.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2805.36 ms\n",
      "llama_print_timings:      sample time =      43.75 ms /    68 runs   (    0.64 ms per token,  1554.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   26202.63 ms /    68 runs   (  385.33 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:       total time =   26295.42 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Holden Caulfield, the protagonist of the book, is a disillusioned teenager who becomes increasingly alienated from society after being expelled from prep school. He spends three days wandering around New York City, reflecting on his past and interactions with various people he meets along the way.']\n"
     ]
    }
   ],
   "source": [
    "query_catcher = 'What happens in the J. D. Salinger novel The Catcher in the Rye?'\n",
    "for index in range(10):\n",
    "    result_catcher, time_catcher, debug_catcher = ask(query=query_catcher, max_tokens=1000)\n",
    "    print(result_catcher)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 144603975,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 761.049422,
   "end_time": "2024-06-19T18:03:33.312939",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-19T17:50:52.263517",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
