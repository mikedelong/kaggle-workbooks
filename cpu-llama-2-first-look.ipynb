{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04efa3f",
   "metadata": {
    "papermill": {
     "duration": 0.003655,
     "end_time": "2024-06-15T20:31:39.135988",
     "exception": false,
     "start_time": "2024-06-15T20:31:39.132333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credit where credit is due: this is based on  https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0681c",
   "metadata": {
    "papermill": {
     "duration": 0.002799,
     "end_time": "2024-06-15T20:31:39.142127",
     "exception": false,
     "start_time": "2024-06-15T20:31:39.139328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the model will take a while so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cd4f36",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-15T20:31:39.152623Z",
     "iopub.status.busy": "2024-06-15T20:31:39.152099Z",
     "iopub.status.idle": "2024-06-15T20:34:10.452259Z",
     "shell.execute_reply": "2024-06-15T20:34:10.450576Z"
    },
    "papermill": {
     "duration": 151.308467,
     "end_time": "2024-06-15T20:34:10.455475",
     "exception": false,
     "start_time": "2024-06-15T20:31:39.147008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=3658479 sha256=a1d0a4fb40265d45e9d32ed21e972e9664a5b6af39173c77fce8872de714915d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\r\n",
      "Cloning into 'llama.cpp'...\r\n",
      "remote: Enumerating objects: 27254, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (5058/5058), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (409/409), done.\u001b[K\r\n",
      "remote: Total 27254 (delta 4787), reused 4785 (delta 4647), pack-reused 22196\u001b[K\r\n",
      "Receiving objects: 100% (27254/27254), 48.72 MiB | 21.98 MiB/s, done.\r\n",
      "Resolving deltas: 100% (19406/19406), done.\r\n"
     ]
    }
   ],
   "source": [
    "# clean up after failed efforts if necessary\n",
    "!rm -rf llama.cpp\n",
    "# we need no CUDA for CPU; this flag has changed since the initial release\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=off\" pip install llama-cpp-python \n",
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f625fd54",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-15T20:34:10.490313Z",
     "iopub.status.busy": "2024-06-15T20:34:10.489877Z",
     "iopub.status.idle": "2024-06-15T20:35:03.963082Z",
     "shell.execute_reply": "2024-06-15T20:35:03.961191Z"
    },
    "papermill": {
     "duration": 53.494314,
     "end_time": "2024-06-15T20:35:03.966234",
     "exception": false,
     "start_time": "2024-06-15T20:34:10.471920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from /kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = 7b-chat-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = 7b-chat-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load time: 53.3105 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '2048', 'general.name': '7b-chat-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "from llama_cpp import Llama\n",
    "\n",
    "time_load = arrow.now()\n",
    "llm = Llama(model_path=\"/kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf\")\n",
    "print(\"Model load time: {:5.4f} seconds\".format((arrow.now() - time_load).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c6d253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T20:35:04.045636Z",
     "iopub.status.busy": "2024-06-15T20:35:04.045082Z",
     "iopub.status.idle": "2024-06-15T20:35:23.916083Z",
     "shell.execute_reply": "2024-06-15T20:35:23.913846Z"
    },
    "papermill": {
     "duration": 19.917309,
     "end_time": "2024-06-15T20:35:23.921997",
     "exception": false,
     "start_time": "2024-06-15T20:35:04.004688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3308.80 ms\n",
      "llama_print_timings:      sample time =      12.63 ms /    19 runs   (    0.66 ms per token,  1504.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3308.61 ms /    16 tokens (  206.79 ms per token,     4.84 tokens per second)\n",
      "llama_print_timings:        eval time =   16494.68 ms /    18 runs   (  916.37 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:       total time =   19844.46 ms /    34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-9bd0f212-c487-4e74-9286-87378bcee5f6', 'object': 'text_completion', 'created': 1718483704, 'model': '/kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf', 'choices': [{'text': '1. Berlin, Germany 2. Paris, France 3. London, United Kingdom', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 18, 'total_tokens': 34}}\n",
      "What are the names of three European capital cities?\n",
      "['1. Berlin, Germany 2. Paris, France 3. London, United Kingdom']\n",
      "time: 19.8575\n"
     ]
    }
   ],
   "source": [
    "def ask(query: str, max_tokens: int):\n",
    "    time_ask = arrow.now()\n",
    "    output = llm(\"Q: {} A: \".format(query), max_tokens=max_tokens, stop=[\"Q:\", \"\\n\"], echo=False)\n",
    "    print(output)\n",
    "    return [output['choices'][index]['text'] for index in range(len(output['choices']))], (arrow.now() - time_ask).total_seconds()\n",
    "\n",
    "query_cities = 'What are the names of three European capital cities?'\n",
    "result_cities, time_cities = ask(query=query_cities, max_tokens=40)\n",
    "print(query_cities)\n",
    "print(result_cities)\n",
    "print('time: {:5.4f}'.format(time_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d89109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T20:35:23.995173Z",
     "iopub.status.busy": "2024-06-15T20:35:23.994679Z",
     "iopub.status.idle": "2024-06-15T20:36:04.449536Z",
     "shell.execute_reply": "2024-06-15T20:36:04.447812Z"
    },
    "papermill": {
     "duration": 40.52592,
     "end_time": "2024-06-15T20:36:04.483987",
     "exception": false,
     "start_time": "2024-06-15T20:35:23.958067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3308.80 ms\n",
      "llama_print_timings:      sample time =      39.97 ms /    65 runs   (    0.61 ms per token,  1626.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4399.71 ms /    25 tokens (  175.99 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =   35909.73 ms /    64 runs   (  561.09 ms per token,     1.78 tokens per second)\n",
      "llama_print_timings:       total time =   40427.93 ms /    89 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-dc144f85-3597-483d-a0a6-aed8231eae70', 'object': 'text_completion', 'created': 1718483723, 'model': '/kaggle/input/test-llama-2-quantized-with-llama-cpp/llama-7b.gguf', 'choices': [{'text': ' The Catcher in the Rye, written by J.D. Salinger in 1951, is a classic coming-of-age story that has become one of the most widely read and influential novels of the 20th century. Here are some key points about the book:', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 65, 'total_tokens': 94}}\n",
      "What can you tell me about the J. D. Salinger novel The Catcher in the Rye?\n",
      "[' The Catcher in the Rye, written by J.D. Salinger in 1951, is a classic coming-of-age story that has become one of the most widely read and influential novels of the 20th century. Here are some key points about the book:']\n",
      "time: 40.4451\n"
     ]
    }
   ],
   "source": [
    "query_catcher = 'What can you tell me about the J. D. Salinger novel The Catcher in the Rye?'\n",
    "result_catcher, time_catcher = ask(query=query_catcher, max_tokens=100)\n",
    "print(query_catcher)\n",
    "print(result_catcher)\n",
    "print('time: {:5.4f}'.format(time_catcher))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 144603975,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 270.957026,
   "end_time": "2024-06-15T20:36:05.348532",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-15T20:31:34.391506",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
